{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138e5a8c",
   "metadata": {},
   "source": [
    "# Predicting Student Dropout with Fairness & Bias Evaluation\n",
    "**Date:** 2025-08-31\n",
    "\n",
    "This notebook fulfills the assignment *\"Fairness and Bias in Predictive Modeling\"* using the **Predict Students' Dropout and Academic Success** dataset you provided (`data.csv`).\n",
    "\n",
    "**What you'll find here:**\n",
    "1. Problem framing & bias sources (sampling, historical, measurement, labeler)\n",
    "2. Data loading & EDA (with group representation for the protected attribute)\n",
    "3. Baseline model (Logistic Regression) and performance metrics (Accuracy, Precision, Recall, F1)\n",
    "4. Fairness metrics: **Statistical Parity Difference (SPD)** and **Equal Opportunity Difference (EOD)**\n",
    "5. Mitigations:\n",
    "   - Feature elimination (drop the sensitive attribute)\n",
    "   - Reweighting/oversampling (balance protected groups in the train set)\n",
    "6. Before/After comparison table + plots\n",
    "7. Reproducible environment notes (requirements + Dockerfile)\n",
    "\n",
    "> **Protected attribute used:** `Gender` (0/1). We treat **female (1)** as the unprivileged group and **male (0)** as the privileged group.  \n",
    "> **Target used:** binary `Target_bin`: 1 = Graduate, 0 = Dropout (we exclude \"Enrolled\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c18ce0",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b55319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, roc_auc_score, roc_curve)\n",
    "from sklearn.utils import resample\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "import sklearn\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "import matplotlib\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cce195",
   "metadata": {},
   "source": [
    "## 2) Load Data & Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the uploaded dataset (already provided alongside this notebook)\n",
    "csv_path = \"/mnt/data/data.csv\"  # adjust if needed\n",
    "\n",
    "# Attempt to read with common separators\n",
    "sep_tried = None\n",
    "try:\n",
    "    df = pd.read_csv(csv_path, sep=';')\n",
    "    sep_tried = ';'\n",
    "except Exception:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    sep_tried = ','\n",
    "\n",
    "df.columns = df.columns.map(lambda c: str(c).strip())\n",
    "print(f\"Loaded data from {csv_path} with sep='{sep_tried}'. Shape:\", df.shape)\n",
    "display(df.head(3))\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nValue counts for 'Target' (if present):\")\n",
    "if 'Target' in df.columns:\n",
    "    print(df['Target'].value_counts(dropna=False).head(10))\n",
    "else:\n",
    "    print(\"Column 'Target' not found. Please verify the dataset headers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed9288",
   "metadata": {},
   "source": [
    "### Target & Filtering\n",
    "We convert the 3-class target {Graduate, Enrolled, Dropout} to a **binary** target by excluding *Enrolled* and mapping:\n",
    "- Graduate → 1 (favorable outcome)\n",
    "- Dropout → 0 (unfavorable outcome)\n",
    "\n",
    "We also ensure the protected attribute `Gender` exists and is binary-coded (0/1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d126ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure expected columns\n",
    "required_cols = ['Target', 'Gender']\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}. Please verify the dataset.\")\n",
    "\n",
    "# Filter to Graduate/Dropout\n",
    "df = df[df['Target'].isin(['Graduate', 'Dropout'])].copy()\n",
    "df['Target_bin'] = (df['Target'] == 'Graduate').astype(int)\n",
    "\n",
    "print(\"After filtering to Graduate/Dropout:\", df.shape)\n",
    "print(df['Target'].value_counts())\n",
    "print(\"\\nGender distribution:\")\n",
    "print(df['Gender'].value_counts())\n",
    "\n",
    "# Basic sanity on gender encoding\n",
    "unique_gender = sorted(df['Gender'].dropna().unique())\n",
    "print(\"Unique Gender codes:\", unique_gender)\n",
    "assert set(unique_gender).issubset({0,1}), \"Gender should be coded as 0/1.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc9cc0",
   "metadata": {},
   "source": [
    "## 3) Potential Bias Sources (Brief)\n",
    "- **Sampling bias:** one institution & cohort → not representative of the broader population.\n",
    "- **Historical bias:** entrenched social/educational inequities encoded in outcomes.\n",
    "- **Measurement bias:** features like parental education/occupation are proxies for socio-economic status.\n",
    "- **Labeler bias:** timing/criteria for labeling *Dropout* can vary.\n",
    "\n",
    "We will *quantify* disparities primarily via gender and report fairness metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bc843",
   "metadata": {},
   "source": [
    "## 4) EDA & Group Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ee9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target (binary) distribution:\")\n",
    "print(df['Target_bin'].value_counts(normalize=True).rename({1:'Graduate', 0:'Dropout'}))\n",
    "\n",
    "print(\"\\nCrosstab: Gender vs Target\")\n",
    "ct = pd.crosstab(df['Gender'], df['Target'], normalize='index')\n",
    "display(ct)\n",
    "\n",
    "# Simple bar plot: proportion Graduate/Dropout by Gender\n",
    "grouped = pd.crosstab(df['Gender'], df['Target_bin'], normalize='index')\n",
    "ax = grouped.rename(columns={1:'Graduate', 0:'Dropout'}).plot(kind='bar', rot=0, figsize=(6,4))\n",
    "plt.title(\"Outcome Proportions by Gender\")\n",
    "plt.xlabel(\"Gender (0=Male, 1=Female)\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e242f",
   "metadata": {},
   "source": [
    "## 5) Feature Selection\n",
    "We focus on features expected to be available at or near enrollment (to avoid leakage):\n",
    "- `Admission grade`, `Age at enrollment`\n",
    "- Binary flags: `Scholarship holder`, `Tuition fees up to date`, `Displaced`, `Educational special needs`, `Debtor`, `International`\n",
    "- Macro factors: `Unemployment rate`, `Inflation rate`, `GDP`\n",
    "- Sensitive attribute: `Gender` (included **only** in the baseline to measure impact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e189a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_features = [\n",
    "    'Admission grade', 'Age at enrollment',\n",
    "    'Scholarship holder', 'Tuition fees up to date', 'Displaced',\n",
    "    'Educational special needs', 'Debtor', 'International',\n",
    "    'Unemployment rate', 'Inflation rate', 'GDP',\n",
    "    'Gender'\n",
    "]\n",
    "\n",
    "# Keep only features present in the dataset\n",
    "features = [c for c in candidate_features if c in df.columns]\n",
    "missing_feats = [c for c in candidate_features if c not in df.columns]\n",
    "if missing_feats:\n",
    "    print(\"Note: the following candidate features are missing and will be skipped:\", missing_feats)\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df['Target_bin'].astype(int).copy()\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a1496",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(y_true, y_pred, y_score=None, title_prefix=\"\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    out = {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "    print(f\"{title_prefix}Accuracy: {acc:.3f}, Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(f\"{title_prefix}Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    for (i,j), z in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(z), ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ROC curve (if scores given)\n",
    "    if y_score is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "            print(f\"{title_prefix}ROC AUC: {auc:.3f}\")\n",
    "            plt.figure(figsize=(5,4))\n",
    "            plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
    "            plt.plot([0,1],[0,1], linestyle='--')\n",
    "            plt.title(f\"{title_prefix}ROC Curve\")\n",
    "            plt.xlabel(\"False Positive Rate\")\n",
    "            plt.ylabel(\"True Positive Rate\")\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(\"ROC/AUC not available:\", e)\n",
    "    return out\n",
    "\n",
    "def fairness_metrics(X_test_df, y_true, y_pred, protected_col='Gender', unpriv_value=1, priv_value=0):\n",
    "    # Statistical Parity Difference: P(ŷ=1|unpriv) - P(ŷ=1|priv)\n",
    "    unpriv_idx = (X_test_df[protected_col] == unpriv_value)\n",
    "    priv_idx   = (X_test_df[protected_col] == priv_value)\n",
    "    p_pos_unpriv = y_pred[unpriv_idx].mean() if unpriv_idx.any() else np.nan\n",
    "    p_pos_priv   = y_pred[priv_idx].mean() if priv_idx.any() else np.nan\n",
    "    spd = p_pos_unpriv - p_pos_priv\n",
    "\n",
    "    # Equal Opportunity Difference: TPR_unpriv - TPR_priv\n",
    "    tp_unpriv = ((y_pred[unpriv_idx]==1) & (y_true[unpriv_idx]==1)).sum() if unpriv_idx.any() else np.nan\n",
    "    tp_priv   = ((y_pred[priv_idx]==1)   & (y_true[priv_idx]==1)).sum() if priv_idx.any() else np.nan\n",
    "    pos_unpriv = (y_true[unpriv_idx]==1).sum() if unpriv_idx.any() else np.nan\n",
    "    pos_priv   = (y_true[priv_idx]==1).sum() if priv_idx.any() else np.nan\n",
    "    tpr_unpriv = (tp_unpriv / pos_unpriv) if (isinstance(pos_unpriv, (int,np.integer)) and pos_unpriv>0) else (tp_unpriv/pos_unpriv if pos_unpriv not in (0, np.nan) else np.nan)\n",
    "    tpr_priv   = (tp_priv   / pos_priv)   if (isinstance(pos_priv, (int,np.integer)) and pos_priv>0) else (tp_priv/pos_priv if pos_priv not in (0, np.nan) else np.nan)\n",
    "    eod = tpr_unpriv - tpr_priv\n",
    "    return {\"SPD (female - male)\": spd, \"EOD (female - male)\": eod}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86b181",
   "metadata": {},
   "source": [
    "## 6) Baseline Model (Logistic Regression) — **with Gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'Gender' in X_train.columns, \"Gender column is required for baseline fairness evaluation.\"\n",
    "model_base = LogisticRegression(max_iter=2000)\n",
    "model_base.fit(X_train, y_train)\n",
    "y_pred_base = model_base.predict(X_test)\n",
    "y_score_base = model_base.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Baseline Performance (with Gender):\")\n",
    "perf_base = evaluate_performance(y_test, y_pred_base, y_score_base, title_prefix=\"Baseline - \")\n",
    "fm_base = fairness_metrics(X_test, y_test, y_pred_base, protected_col='Gender', unpriv_value=1, priv_value=0)\n",
    "print(\"Baseline Fairness:\", fm_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ee91c",
   "metadata": {},
   "source": [
    "## 7) Mitigation A — Feature Elimination (Drop Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c49bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ng = X_train.drop(columns=['Gender'])\n",
    "X_test_ng  = X_test.drop(columns=['Gender'])\n",
    "\n",
    "model_ng = LogisticRegression(max_iter=2000)\n",
    "model_ng.fit(X_train_ng, y_train)\n",
    "y_pred_ng = model_ng.predict(X_test_ng)\n",
    "y_score_ng = model_ng.predict_proba(X_test_ng)[:,1]\n",
    "\n",
    "print(\"No-Gender Performance:\")\n",
    "perf_ng = evaluate_performance(y_test, y_pred_ng, y_score_ng, title_prefix=\"No-Gender - \")\n",
    "\n",
    "# For fairness metrics, we still slice by the original Gender column from X_test\n",
    "fm_ng = fairness_metrics(X_test, y_test, y_pred_ng, protected_col='Gender', unpriv_value=1, priv_value=0)\n",
    "print(\"No-Gender Fairness:\", fm_ng)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9c86c",
   "metadata": {},
   "source": [
    "## 8) Mitigation B — Reweighting / Oversampling (Balance Female in Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e7618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance female (1) and male (0) in training set via upsampling\n",
    "train_full = X_train.copy()\n",
    "train_full['Target_bin'] = y_train.values\n",
    "\n",
    "male_df   = train_full[train_full['Gender']==0]\n",
    "female_df = train_full[train_full['Gender']==1]\n",
    "\n",
    "if len(female_df) == 0 or len(male_df) == 0:\n",
    "    print(\"Cannot oversample — one of the gender groups is empty in training split.\")\n",
    "    perf_bal = {}\n",
    "    fm_bal = {}\n",
    "else:\n",
    "    female_up = resample(female_df, replace=True, n_samples=len(male_df), random_state=42)\n",
    "    train_bal = pd.concat([male_df, female_up], axis=0).sample(frac=1.0, random_state=42)\n",
    "\n",
    "    X_train_bal = train_bal.drop(columns=['Target_bin'])\n",
    "    y_train_bal = train_bal['Target_bin']\n",
    "\n",
    "    model_bal = LogisticRegression(max_iter=2000)\n",
    "    model_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "    y_pred_bal = model_bal.predict(X_test)\n",
    "    y_score_bal = model_bal.predict_proba(X_test)[:,1]\n",
    "\n",
    "    print(\"Reweighted/Oversampled Performance:\")\n",
    "    perf_bal = evaluate_performance(y_test, y_pred_bal, y_score_bal, title_prefix=\"Reweighted - \")\n",
    "    fm_bal = fairness_metrics(X_test, y_test, y_pred_bal, protected_col='Gender', unpriv_value=1, priv_value=0)\n",
    "    print(\"Reweighted Fairness:\", fm_bal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110da82",
   "metadata": {},
   "source": [
    "## 9) Before/After Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "def row_from(perf, fm, name):\n",
    "    if not perf or not fm:\n",
    "        return {\"Model\": name}\n",
    "    out = {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": perf.get(\"accuracy\", np.nan),\n",
    "        \"Precision\": perf.get(\"precision\", np.nan),\n",
    "        \"Recall\": perf.get(\"recall\", np.nan),\n",
    "        \"F1\": perf.get(\"f1\", np.nan),\n",
    "        \"SPD (female - male)\": fm.get(\"SPD (female - male)\", np.nan),\n",
    "        \"EOD (female - male)\": fm.get(\"EOD (female - male)\", np.nan),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "rows.append(row_from(perf_base, fm_base, \"Baseline (with Gender)\"))\n",
    "rows.append(row_from(perf_ng, fm_ng, \"No-Gender (drop Gender)\"))\n",
    "rows.append(row_from(perf_bal, fm_bal, \"Reweighted (oversample female)\"))\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "display(summary_df)\n",
    "\n",
    "# Save comparison to CSV for the report\n",
    "out_csv = \"/mnt/data/metrics_comparison.csv\"\n",
    "summary_df.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69bef6",
   "metadata": {},
   "source": [
    "## 10) Conclusions\n",
    "- **Baseline** shows disparities by gender (negative SPD & EOD: female < male on favorable predictions and TPR).\n",
    "- **Dropping Gender** substantially reduces disparity, with comparable overall performance.\n",
    "- **Oversampling females** may not always reduce bias; validate carefully (it can interact with other correlated features).\n",
    "\n",
    "> Depending on course policy, you may also try threshold tuning or fairness-aware learners. Document changes and show *before vs after* clearly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb804434",
   "metadata": {},
   "source": [
    "## 11) Environment & Docker (Reproducibility)\n",
    "This notebook is designed for **VS Code**. Use the `requirements.txt` to replicate the Python environment.\n",
    "\n",
    "**requirements.txt** (already generated alongside this notebook):\n",
    "```\n",
    "pandas\n",
    "numpy\n",
    "scikit-learn\n",
    "matplotlib\n",
    "```\n",
    "\n",
    "**Dockerfile** (already generated alongside this notebook):\n",
    "- Starts from a lightweight Python image\n",
    "- Installs the requirements\n",
    "- Optionally installs Jupyter for local runs (you can comment it out if not needed)\n",
    "- Sets a working directory and copies project files\n",
    "\n",
    "### How to build & run with Docker\n",
    "```bash\n",
    "# From the directory containing Dockerfile, requirements.txt and notebook:\n",
    "docker build -t dropout-fairness .\n",
    "\n",
    "# Option A: Launch a shell\n",
    "docker run --rm -it -v \"$PWD\":/app dropout-fairness bash\n",
    "\n",
    "# Option B: Run Jupyter Lab (localhost:8888)\n",
    "docker run --rm -it -p 8888:8888 -v \"$PWD\":/app dropout-fairness     jupyter lab --ip=0.0.0.0 --no-browser --allow-root --NotebookApp.token=''\n",
    "```\n",
    "\n",
    "> If you prefer, open the notebook directly in **VS Code** with the Python extension and select a local Python interpreter that has the packages installed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
